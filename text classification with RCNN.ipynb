{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 11:40:29.888482 140058632382272 file_utils.py:32] TensorFlow version 2.1.0 available.\n",
      "I0401 11:40:29.889459 140058632382272 file_utils.py:39] PyTorch version 1.4.0 available.\n",
      "I0401 11:40:30.201050 140058632382272 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "max_seq_len = 50\n",
    "TEXT = data.Field(tokenize=\"spacy\", batch_first=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('label', LABEL), (None, None), ('text',TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=data.TabularDataset(path = 'AG_news/train.csv',format = 'csv',fields = fields,skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '3', 'text': ['Reuters', '-', 'Short', '-', 'sellers', ',', 'Wall', 'Street', \"'s\", 'dwindling\\\\band', 'of', 'ultra', '-', 'cynics', ',', 'are', 'seeing', 'green', 'again', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = training_data.split(split_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 11:41:13.653469 140058632382272 vocab.py:431] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 11451\n",
      "Size of LABEL vocabulary: 4\n",
      "[('the', 17637), (',', 14374), ('.', 13166), ('-', 9804), ('to', 9667), ('a', 9448), ('of', 8929), ('in', 7588), ('and', 6643), ('on', 4687)]\n"
     ]
    }
   ],
   "source": [
    "#initialize glove embeddings\n",
    "TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.300d\")  \n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  \n",
    "\n",
    "#Word dictionary\n",
    "# print(TEXT.vocab.stoi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data), batch_size=batch_size,\n",
    "                                                           sort_key=lambda x: len(x.text),\n",
    "                                                           sort_within_batch=True,\n",
    "                                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network representation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RCNNTextClassification(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_size, hidden_size_lstm, hidden_size_linear,\n",
    "                 num_layers, bidirectional,\n",
    "                 max_seq_len, dropout, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size_lstm = hidden_size_lstm\n",
    "        self.hidden_size_linear = hidden_size_linear\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.embedding_size, self.hidden_size_lstm, num_layers, bidirectional=bidirectional,\n",
    "                          dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.W = nn.Linear(self.embedding_size + 2*self.hidden_size_lstm, self.hidden_size_linear)\n",
    "            \n",
    "        else:\n",
    "            self.W = nn.Linear(self.embedding_size + self.hidden_size_lstm, self.hidden_size_linear)\n",
    "            \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc = nn.Linear(self.hidden_size_linear, num_class)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        emb = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(emb, text_lengths, batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        x, lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        input_features = torch.cat([x, emb], 2)\n",
    "        linear_output = self.tanh(self.W(input_features))\n",
    "        \n",
    "        linear_output = linear_output.permute(0,2,1) # Reshaping fot max_pool\n",
    "        \n",
    "        max_out_features = F.max_pool1d(linear_output, linear_output.shape[2]).squeeze(2)\n",
    "        \n",
    "        max_out_features = self.dropout(max_out_features)\n",
    "        final_out = self.fc(max_out_features)\n",
    "        \n",
    "        return self.softmax(final_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(TEXT.vocab)\n",
    "n_class = len(LABEL.vocab)\n",
    "embedding_size = 300\n",
    "hidden_size_lstm = 75\n",
    "hidden_size_linear = 75\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.6\n",
    "\n",
    "model = RCNNTextClassification(vocabulary_size, embedding_size, hidden_size_lstm, hidden_size_linear, num_layers, bidirectional,\n",
    "                 max_seq_len, dropout, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RCNNTextClassification(\n",
       "  (embedding): Embedding(11451, 300)\n",
       "  (rnn): LSTM(300, 75, batch_first=True, dropout=0.6, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.6, inplace=False)\n",
       "  (W): Linear(in_features=450, out_features=75, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (fc): Linear(in_features=75, out_features=4, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 4.6560e-02,  2.1318e-01, -7.4364e-03,  ...,  9.0611e-03,\n",
       "         -2.0989e-01,  5.3913e-02],\n",
       "        ...,\n",
       "        [-5.3067e-01,  3.0893e-02,  3.2893e-01,  ...,  3.0950e-01,\n",
       "          3.6670e-01, -2.9955e-01],\n",
       "        [-4.1813e-01,  4.8057e-01,  4.9454e-01,  ...,  4.2032e-04,\n",
       "         -3.9901e-01,  1.8607e-01],\n",
       "        [-4.8421e-01,  2.5875e-01, -9.6700e-02,  ..., -7.2311e-01,\n",
       "          2.3865e-01,  3.1459e-01]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, iterator, optimizer, criterion):\n",
    "    training_loss = 0\n",
    "    training_accuracy = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        \n",
    "        output = model(text, text_lengths).squeeze()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        num_corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "\n",
    "        acc = num_corrects/len(batch)\n",
    "        training_accuracy += acc.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    return training_loss / len(iterator), training_accuracy / len(iterator)\n",
    "\n",
    "def testing(model, iterator, optimizer, criterion):\n",
    "    testing_loss = 0\n",
    "    testing_accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(text, text_lengths).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            testing_loss += loss.item()\n",
    "            num_corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "            acc = num_corrects/len(batch)\n",
    "        \n",
    "            testing_accuracy += acc.item()\n",
    "            \n",
    "    return testing_loss / len(iterator), testing_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 1 minutes, 48 seconds\n",
      "\tLoss: 0.6566(train)\t|\tAcc: 75.63%(train)\n",
      "\tLoss: 0.4725(valid)\t|\tAcc: 83.16%(valid)\n",
      "Epoch: 2  | time in 1 minutes, 42 seconds\n",
      "\tLoss: 0.5056(train)\t|\tAcc: 82.33%(train)\n",
      "\tLoss: 0.4334(valid)\t|\tAcc: 84.68%(valid)\n",
      "Epoch: 3  | time in 1 minutes, 35 seconds\n",
      "\tLoss: 0.4509(train)\t|\tAcc: 84.25%(train)\n",
      "\tLoss: 0.4262(valid)\t|\tAcc: 85.07%(valid)\n",
      "Epoch: 4  | time in 1 minutes, 37 seconds\n",
      "\tLoss: 0.4243(train)\t|\tAcc: 84.96%(train)\n",
      "\tLoss: 0.4171(valid)\t|\tAcc: 85.37%(valid)\n",
      "Epoch: 5  | time in 1 minutes, 32 seconds\n",
      "\tLoss: 0.3846(train)\t|\tAcc: 86.39%(train)\n",
      "\tLoss: 0.3960(valid)\t|\tAcc: 86.21%(valid)\n",
      "Epoch: 6  | time in 1 minutes, 37 seconds\n",
      "\tLoss: 0.3616(train)\t|\tAcc: 87.27%(train)\n",
      "\tLoss: 0.3951(valid)\t|\tAcc: 86.33%(valid)\n",
      "Epoch: 7  | time in 1 minutes, 38 seconds\n",
      "\tLoss: 0.3373(train)\t|\tAcc: 88.24%(train)\n",
      "\tLoss: 0.3976(valid)\t|\tAcc: 86.25%(valid)\n",
      "Epoch: 8  | time in 1 minutes, 35 seconds\n",
      "\tLoss: 0.3175(train)\t|\tAcc: 88.71%(train)\n",
      "\tLoss: 0.3836(valid)\t|\tAcc: 86.64%(valid)\n",
      "Epoch: 9  | time in 1 minutes, 33 seconds\n",
      "\tLoss: 0.2979(train)\t|\tAcc: 89.58%(train)\n",
      "\tLoss: 0.3865(valid)\t|\tAcc: 86.89%(valid)\n",
      "Epoch: 10  | time in 1 minutes, 35 seconds\n",
      "\tLoss: 0.2851(train)\t|\tAcc: 89.95%(train)\n",
      "\tLoss: 0.3881(valid)\t|\tAcc: 86.69%(valid)\n",
      "Epoch: 11  | time in 1 minutes, 37 seconds\n",
      "\tLoss: 0.2669(train)\t|\tAcc: 90.92%(train)\n",
      "\tLoss: 0.3818(valid)\t|\tAcc: 87.11%(valid)\n",
      "Epoch: 12  | time in 1 minutes, 32 seconds\n",
      "\tLoss: 0.2504(train)\t|\tAcc: 91.26%(train)\n",
      "\tLoss: 0.3851(valid)\t|\tAcc: 87.06%(valid)\n",
      "Epoch: 13  | time in 1 minutes, 26 seconds\n",
      "\tLoss: 0.2417(train)\t|\tAcc: 91.49%(train)\n",
      "\tLoss: 0.3920(valid)\t|\tAcc: 86.89%(valid)\n",
      "Epoch: 14  | time in 1 minutes, 30 seconds\n",
      "\tLoss: 0.2302(train)\t|\tAcc: 91.94%(train)\n",
      "\tLoss: 0.3874(valid)\t|\tAcc: 87.09%(valid)\n",
      "Epoch: 15  | time in 1 minutes, 30 seconds\n",
      "\tLoss: 0.2230(train)\t|\tAcc: 92.05%(train)\n",
      "\tLoss: 0.3916(valid)\t|\tAcc: 86.96%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 15\n",
    "min_val_loss = float(\"inf\")\n",
    "path='AG_news/model/saved_weights_rcnn.pt'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = training(model, train_iterator, optimizer, criterion)\n",
    "    val_loss, val_acc = testing(model, valid_iterator, optimizer, criterion)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
    "    print(f'\\tLoss: {val_loss:.4f}(valid)\\t|\\tAcc: {val_acc * 100:.2f}%(valid)')\n",
    "    \n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data=data.TabularDataset(path = 'AG_news/test.csv',format = 'csv',fields = fields,skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_iterator = data.BucketIterator(testing_data, batch_size=batch_size,\n",
    "                                                           sort_key=lambda x: len(x.text),\n",
    "                                                           sort_within_batch=True,\n",
    "                                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "def predict(model, iterator):\n",
    "    testing_accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        # text = TEXT.preprocess(text)\n",
    "        label = batch.label\n",
    "        target = torch.autograd.Variable(label).long()\n",
    "        with torch.no_grad():\n",
    "            output = model(text, text_lengths).squeeze()\n",
    "            num_corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "            acc = num_corrects / len(batch)\n",
    "            testing_accuracy += acc.item()\n",
    "    \n",
    "    return testing_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 86.58\n"
     ]
    }
   ],
   "source": [
    "test_acc = predict(model, testing_iterator)\n",
    "print(f\"Accuracy {test_acc * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
