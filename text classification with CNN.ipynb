{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "max_seq_len = 50\n",
    "TEXT = data.Field(tokenize=\"spacy\", batch_first=True, include_lengths=True, fix_length=max_seq_len)\n",
    "LABEL = data.LabelField(dtype=torch.float, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('label', LABEL), (None, None), ('text',TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=data.TabularDataset(path = 'AG_news/train.csv',format = 'csv',fields = fields,skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '3', 'text': ['Reuters', '-', 'Short', '-', 'sellers', ',', 'Wall', 'Street', \"'s\", 'dwindling\\\\band', 'of', 'ultra', '-', 'cynics', ',', 'are', 'seeing', 'green', 'again', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = training_data.split(split_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0401 08:49:42.976831 140345299633984 vocab.py:431] Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 11553\n",
      "Size of LABEL vocabulary: 4\n",
      "[('the', 17355), (',', 14475), ('.', 13211), ('-', 9804), ('a', 9516), ('to', 9420), ('of', 8873), ('in', 7689), ('and', 6488), ('on', 4694)]\n"
     ]
    }
   ],
   "source": [
    "#initialize glove embeddings\n",
    "TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.300d\")  \n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  \n",
    "\n",
    "#Word dictionary\n",
    "# print(TEXT.vocab.stoi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data), batch_size=batch_size,\n",
    "                                                           sort_key=lambda x: len(x.text),\n",
    "                                                           sort_within_batch=True,\n",
    "                                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network representation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CNNTextClassification(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_size, max_seq_len, out_channels,\n",
    "                 kernel_heights, dropout, num_class):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_heights = kernel_heights\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=self.embedding_size, out_channels=self.out_channels,\n",
    "                               kernel_size=self.kernel_heights[0]),\n",
    "                                   nn.ReLU(),\n",
    "                                  nn.MaxPool1d(self.max_seq_len - self.kernel_heights[0]+1))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=self.embedding_size, out_channels=self.out_channels,\n",
    "                               kernel_size=self.kernel_heights[1]),\n",
    "                                   nn.ReLU(),\n",
    "                                  nn.MaxPool1d(self.max_seq_len - self.kernel_heights[1]+1))\n",
    "        \n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=self.embedding_size, out_channels=self.out_channels,\n",
    "                               kernel_size=self.kernel_heights[2]),\n",
    "                                   nn.ReLU(),\n",
    "                                  nn.MaxPool1d(self.max_seq_len - self.kernel_heights[2]+1))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(len(self.kernel_heights) * out_channels, num_class)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        emb = self.embedding(text).permute(0, 2, 1)\n",
    "        \n",
    "        conv_out1 = self.conv1(emb).squeeze(2)\n",
    "        conv_out2 = self.conv2(emb).squeeze(2)\n",
    "        conv_out3 = self.conv3(emb).squeeze(2)\n",
    "        \n",
    "        all_out = torch.cat((conv_out1, conv_out2, conv_out3), 1)\n",
    "        final_feature_map = self.dropout(all_out)\n",
    "        \n",
    "        final_out = self.fc(final_feature_map)\n",
    "        \n",
    "        return self.softmax(final_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(TEXT.vocab)\n",
    "n_class = len(LABEL.vocab)\n",
    "embedding_size = 300\n",
    "out_channels = 100\n",
    "kernel_heights = [3, 4, 5]\n",
    "dropout = 0.4\n",
    "\n",
    "model = CNNTextClassification(vocabulary_size, embedding_size, max_seq_len,\n",
    "                              out_channels, kernel_heights, dropout, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNTextClassification(\n",
       "  (embedding): Embedding(11553, 300)\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=48, stride=48, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=46, stride=46, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (fc): Linear(in_features=300, out_features=4, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
       "        ...,\n",
       "        [-0.4171,  0.2261, -0.0720,  ..., -0.8584,  0.2861,  0.1984],\n",
       "        [-0.2022,  0.2169, -0.1251,  ..., -0.2296, -0.1567,  0.1279],\n",
       "        [ 0.4281,  0.4327,  0.0331,  ..., -0.3796,  0.2552,  0.0976]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, iterator, optimizer, criterion):\n",
    "    training_loss = 0\n",
    "    training_accuracy = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        \n",
    "        output = model(text, text_lengths).squeeze()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        num_corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "\n",
    "        acc = num_corrects/len(batch)\n",
    "        training_accuracy += acc.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    return training_loss / len(iterator), training_accuracy / len(iterator)\n",
    "\n",
    "def testing(model, iterator, optimizer, criterion):\n",
    "    testing_loss = 0\n",
    "    testing_accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(text, text_lengths).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            testing_loss += loss.item()\n",
    "            num_corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "            acc = num_corrects/len(batch)\n",
    "        \n",
    "            testing_accuracy += acc.item()\n",
    "            \n",
    "    return testing_loss / len(iterator), testing_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 1 minutes, 21 seconds\n",
      "\tLoss: 0.9035(train)\t|\tAcc: 62.52%(train)\n",
      "\tLoss: 0.7674(valid)\t|\tAcc: 73.22%(valid)\n",
      "Epoch: 2  | time in 1 minutes, 21 seconds\n",
      "\tLoss: 0.8207(train)\t|\tAcc: 66.64%(train)\n",
      "\tLoss: 0.7223(valid)\t|\tAcc: 75.11%(valid)\n",
      "Epoch: 3  | time in 1 minutes, 24 seconds\n",
      "\tLoss: 0.7772(train)\t|\tAcc: 68.95%(train)\n",
      "\tLoss: 0.7072(valid)\t|\tAcc: 76.05%(valid)\n",
      "Epoch: 4  | time in 1 minutes, 24 seconds\n",
      "\tLoss: 0.7483(train)\t|\tAcc: 70.00%(train)\n",
      "\tLoss: 0.6821(valid)\t|\tAcc: 77.07%(valid)\n",
      "Epoch: 5  | time in 1 minutes, 22 seconds\n",
      "\tLoss: 0.7222(train)\t|\tAcc: 71.54%(train)\n",
      "\tLoss: 0.7072(valid)\t|\tAcc: 74.87%(valid)\n",
      "Epoch: 6  | time in 1 minutes, 26 seconds\n",
      "\tLoss: 0.6992(train)\t|\tAcc: 72.37%(train)\n",
      "\tLoss: 0.6687(valid)\t|\tAcc: 77.43%(valid)\n",
      "Epoch: 7  | time in 1 minutes, 23 seconds\n",
      "\tLoss: 0.6664(train)\t|\tAcc: 74.88%(train)\n",
      "\tLoss: 0.6474(valid)\t|\tAcc: 77.86%(valid)\n",
      "Epoch: 8  | time in 1 minutes, 22 seconds\n",
      "\tLoss: 0.6397(train)\t|\tAcc: 76.43%(train)\n",
      "\tLoss: 0.6401(valid)\t|\tAcc: 78.34%(valid)\n",
      "Epoch: 9  | time in 1 minutes, 22 seconds\n",
      "\tLoss: 0.6208(train)\t|\tAcc: 77.25%(train)\n",
      "\tLoss: 0.6349(valid)\t|\tAcc: 78.50%(valid)\n",
      "Epoch: 10  | time in 1 minutes, 25 seconds\n",
      "\tLoss: 0.6062(train)\t|\tAcc: 77.70%(train)\n",
      "\tLoss: 0.6306(valid)\t|\tAcc: 79.03%(valid)\n",
      "Epoch: 11  | time in 1 minutes, 25 seconds\n",
      "\tLoss: 0.6008(train)\t|\tAcc: 78.49%(train)\n",
      "\tLoss: 0.6347(valid)\t|\tAcc: 78.76%(valid)\n",
      "Epoch: 12  | time in 1 minutes, 21 seconds\n",
      "\tLoss: 0.5886(train)\t|\tAcc: 78.86%(train)\n",
      "\tLoss: 0.6276(valid)\t|\tAcc: 79.51%(valid)\n",
      "Epoch: 13  | time in 1 minutes, 22 seconds\n",
      "\tLoss: 0.5775(train)\t|\tAcc: 79.15%(train)\n",
      "\tLoss: 0.6254(valid)\t|\tAcc: 79.63%(valid)\n",
      "Epoch: 14  | time in 1 minutes, 23 seconds\n",
      "\tLoss: 0.5822(train)\t|\tAcc: 78.89%(train)\n",
      "\tLoss: 0.6261(valid)\t|\tAcc: 79.28%(valid)\n",
      "Epoch: 15  | time in 1 minutes, 25 seconds\n",
      "\tLoss: 0.5644(train)\t|\tAcc: 79.76%(train)\n",
      "\tLoss: 0.6314(valid)\t|\tAcc: 79.43%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 15\n",
    "min_val_loss = float(\"inf\")\n",
    "path='AG_news/model/saved_weights_cnn.pt'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = training(model, train_iterator, optimizer, criterion)\n",
    "    val_loss, val_acc = testing(model, valid_iterator, optimizer, criterion)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
    "    print(f'\\tLoss: {val_loss:.4f}(valid)\\t|\\tAcc: {val_acc * 100:.2f}%(valid)')\n",
    "    \n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data=data.TabularDataset(path = 'AG_news/test.csv',format = 'csv',fields = fields,skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_iterator = data.BucketIterator(testing_data, batch_size=batch_size,\n",
    "                                                           sort_key=lambda x: len(x.text),\n",
    "                                                           sort_within_batch=True,\n",
    "                                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "def predict(model, iterator):\n",
    "    testing_accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        # text = TEXT.preprocess(text)\n",
    "        label = batch.label\n",
    "        target = torch.autograd.Variable(label).long()\n",
    "        with torch.no_grad():\n",
    "            output = model(text, text_lengths).squeeze()\n",
    "            num_corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "            acc = num_corrects / len(batch)\n",
    "            testing_accuracy += acc.item()\n",
    "    \n",
    "    return testing_accuracy / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 79.84\n"
     ]
    }
   ],
   "source": [
    "test_acc = predict(model, testing_iterator)\n",
    "print(f\"Accuracy {test_acc * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
